{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AryanAhmadChaudhary/Mini-RLHF-DPO-Alignment/blob/main/dpo_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URzEBawdO32y"
      },
      "source": [
        "#imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OIeefWYDjllS"
      },
      "outputs": [],
      "source": [
        "!pip install trl transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRHYLYDIjXUT"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from trl import DPOConfig, DPOTrainer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXdGc0z_O7_I"
      },
      "source": [
        "#load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rxpaR7Xujekl"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"shawhin/youtube-titles-dpo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7p8Q8OVO_NX"
      },
      "source": [
        "#load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "u_AF67LRjgZq"
      },
      "outputs": [],
      "source": [
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ5dp0COPB60"
      },
      "source": [
        "#generate title with base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FETVwZprjiYM"
      },
      "outputs": [],
      "source": [
        "def format_chat_prompt(user_input, system_message=\"You are a helpful assistant.\"):\n",
        "    \"\"\"\n",
        "    Formats user input into the chat template format with <|im_start|> and <|im_end|> tags.\n",
        "\n",
        "    Args:\n",
        "        user_input (str): The input text from the user.\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted prompt for the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Format user message\n",
        "    user_prompt = f\"<|im_start|>user\\n{user_input}<|im_end|>\\n\"\n",
        "\n",
        "    # Start assistant's turn\n",
        "    assistant_prompt = \"<|im_start|>assistant\\n\"\n",
        "\n",
        "    # Combine prompts\n",
        "    formatted_prompt = user_prompt + assistant_prompt\n",
        "\n",
        "    return formatted_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHeBgQJ7jq99"
      },
      "outputs": [],
      "source": [
        "# Set up text generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Example prompt\n",
        "prompt = format_chat_prompt(dataset['valid']['prompt'][0][0]['content'])\n",
        "\n",
        "# Generate output\n",
        "outputs = generator(prompt, max_length=100, truncation=True, num_return_sequences=1, temperature=0.7)\n",
        "\n",
        "print(outputs[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBF-jLpSPF3J"
      },
      "source": [
        "#train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZBekwBdjtFU"
      },
      "outputs": [],
      "source": [
        "ft_model_name = model_name.split('/')[1].replace(\"Instruct\", \"DPO\")\n",
        "\n",
        "training_args = DPOConfig(\n",
        "    output_dir=ft_model_name,\n",
        "    logging_steps=25,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    eval_steps=1,\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpP-ElDejvRH"
      },
      "outputs": [],
      "source": [
        "trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['valid'],\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hu_5laMPIro"
      },
      "source": [
        "#use fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkKlE9qijxMr"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned model\n",
        "ft_model = trainer.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHiGIE28jy_Q"
      },
      "outputs": [],
      "source": [
        "# Set up text generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=ft_model, tokenizer=tokenizer)\n",
        "\n",
        "# Example prompt\n",
        "prompt = format_chat_prompt(dataset['valid']['prompt'][0][0]['content'])\n",
        "\n",
        "# Generate output\n",
        "outputs = generator(prompt, max_length=100, truncation=True, num_return_sequences=1, temperature=0.7)\n",
        "\n",
        "print(outputs[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNLp3FXxj4qQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}